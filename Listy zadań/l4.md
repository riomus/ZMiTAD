Lista 4 - Odkrywanie reguł asocjacyjnych
==========

**Spark**
 - [Wprowadzenie](http://spark.apache.org/docs/latest/)
 - [Download](http://spark.apache.org/downloads.html)


Zadania mogą być realizowane na wiele sposobów, te które będą wśród państwa najpopularniejsze to:

1. Przyjemny :relaxed: :

   1. Projekt SBT w IDE,
   2. Uruchamianie kodu z poziomu IDE,
   3. Podpowiadanie składni, 
   4. Dostęp do dokumentacji,

2. Mniej przyjemny :rage: :

   1. Nieuporządkowany kod w notatniku,
   2. Konieczność instalacji Sparka,
   4. Wklejanie kodu do CLI,
   5. Marnowanie czasu,

IDE
-----
1. Dołącz do programu studenckiego IDEA - [klik](https://www.jetbrains.com/student/)
2. Pobierz InteliJ
3. Zainstaluj wtyczkę do Scali - [klik](https://www.jetbrains.com/help/idea/2016.2/creating-and-running-your-scala-application.html)
4. Podczas importu projektu, InteliJ sam poprosi o instalację potrzebnych rzeczy (Scala, SBT itd.)


Instalacja Sparka
-------------------

**Nie jest to krok wymagany!**

1. Zainstaluj Jave (najelpiej w wersji 1.8)
2. Pobierz archiwum - [download](http://spark.apache.org/downloads.html)
3. Wypakuj archiwum
4. Ponieważ nie będziemy korzystać ze Sparka uruchamianego na wielu węzłach, instalacja jest zakończona
5. Test: uruchum CLI (spark-2.0.1-bin-hadoop2.7/bin/spark-shell) i wykonaj poniższy kod:

   ```scala
   sc.parallelize((1 to 100).toSeq)
      .flatMap(_=>(1 to 100).iterator)
      .map(_*10)
      .reduce((a,b)=>a+b)
   ```

Projekt (kod)
--------------

1. Pobierz projekt - [klik](https://github.com/riomus/spark-start)
2. Zaimportuj projekt do IDE
3. Uruchom projekt z poziomu IDE (obiekt SparkStart w src/main/pl/riomus)
4. W ten sposób cały kod potrzebny do zrealizowania listy może zostać umieszczony w tym obiekcie (bądź oddzielnych, podzielonych ze względu na zadania, odpowiedzialności itd.)
5. Jeżeli zadanie jest obliczane długo, można sprawdzić co jest aktualnie robione na web UI udostępnionym na porcie 4040 (przykładowy kod jest szybki)
6. Jeżeli korzystają państwo z systemu Windows, proszę o [instalację biblioteki hadoop](http://teknosrc.com/spark-error-java-io-ioexception-could-not-locate-executable-null-bin-winutils-exe-hadoop-binaries/) ([stąd pobrać](https://github.com/srccodes/hadoop-common-2.2.0-bin)) oraz dodanie linijki kodu:

  ```scala
  System.setProperty("hadoop.home.dir", "<PATH_TO_HADOOP>");
  ```


Reguły asocjacyjne
-------------------

1. [W Sparku](http://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html)
2. [FP-growth](http://www.dei.unipd.it/~capri/DATAMINING/PAPERS/HanPY00.pdf.gz)
3. [Wiki](https://en.wikipedia.org/wiki/Association_rule_learning)
3. [W R](https://www.r-bloggers.com/association-rule-learning-and-the-apriori-algorithm/)


Przydatny kod do R
--------------------

```R
install.packages("arules");
install.packages("arulesViz");

library("arules");
library("arulesViz");

data(package="arules");
data("Groceries");

rules = apriori(Groceries, parameter=list(support=0.01, confidence=0.5));

inspect(head(sort(rules, by="lift"),3));
plot(rules);
head(quality(rules));
plot(rules, measure=c("support","lift"), shading="confidence");
plot(rules, shading="order", control=list(main ="Two-key plot"));
```


Zadania
----------


1. Wybierz zbiór danych do przetwarzania
  
  [proponowany](https://sites.google.com/a/nu.edu.pk/tariq-mahmood/teaching-1/fall-12---dm/marketbasket.csv?attredirects=0&d=1)

2. Przeprowadź prostą analizę zbioru danych, np.:
   
  1. Ile jest wszystkich koszyków
  2. Ile jest produktów
  3. Jak często średnio pojawia się produkt
  4. Który produkt pojawia się najczęściej w koszyku
  5. ...

  Przydatny kod (dla pliku przykładowego)

  ```scala
    val fileLoaded=ctx.textFile("file_path")
    .map(line=>line.split("delimiter"))
    .map(elementsInLine=>elementsInLine.map(_.trim))
    .zipWithIndex()

    val header=fileLoaded.first()._1
    val restOfFile=fileLoaded.filter(_._2>0).map(_._1)

    val howManyFirstProduct=restOfFile.map(_(1).toBoolean).map(n=>if(n) 1 else 0).sum()
  ```

  **Środowisko:** Dowolne (**Spark**, Matlab, R, Python, Java, MIPS assembly ...)

3. Wykonaj wykrywanie zbiorów częstych z wykorzystaniem algorytmu FP-growth

  1. Dokonaj odpowiedniej transformacji zbioru danych (przykładowy plik (RDD po załadowaniu) składa się z linii zawierających wartości true/false zależnie od tego czy produkt jest w koszyku, algorytm *FPGrowth* przyjmuje RDD skłądające się z tablic, w których każda składa się z **nazw** produktów )
  2. Uruchom algorytm *FPGrowth*
  3. Przeanalizuj wyniki np.
    1. Ciekawe asocjacje
    2. Wyniki oczywiste
    3. ...

  **Środowisko:** Dowolne (**Spark**, Matlab, R, Python, Java, MIPS assembly ...)

4.  Wykonaj generowanie reguł asocjacyjnych dla dowolnego zbioru (może być zbiór z poprzedniego zadania)

  1. Załaduj i przerób na odpowiedni format zbiór danych
  2. Dokonaj odkrywania reguł asocjacyjnych dla różnych zestawów (conajmniej 5) parametrów wsparcia i pewności
  3. Dokonaj anlizy otrzymanych wynikó

  **Środowisko:** R

